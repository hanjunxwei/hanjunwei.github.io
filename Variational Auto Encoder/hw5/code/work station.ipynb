{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cb71d0",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec99252",
   "metadata": {},
   "source": [
    "**Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac9f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "from tensorflow.math import exp, sqrt, square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff0ea1",
   "metadata": {},
   "source": [
    "**VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c400f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, input_size, latent_size=15):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size # H*W  Original dimention\n",
    "        self.latent_size = latent_size  # Z \n",
    "        \n",
    "        self.hidden_dim = 200  # H_d up to you\n",
    "        \n",
    "        self.encoder = Sequential()\n",
    "        self.mu_layer = Dense(self.latent_size)\n",
    "        self.logvar_layer = Dense(self.latent_size)\n",
    "        self.decoder = Sequential()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder.add(Flatten(input_shape = (28,28))) # input_shape = self.input_size (int(sqrt(self.input_size)),int(sqrt(self.input_size)))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        \n",
    "\n",
    "        # Decoder\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.input_size, activation = 'sigmoid'))\n",
    "        self.decoder.add(Reshape((1,28,28)))\n",
    "        \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through FC-VAE model by passing image through \n",
    "        encoder, reparametrize trick, and decoder models\n",
    "    \n",
    "        Inputs:\n",
    "        - x: Batch of input images of shape (N, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "        - x_hat: Reconstruced input data of shape (N,1,H,W)\n",
    "        - mu: Matrix representing estimated posterior mu (N, Z), with Z latent space dimension\n",
    "        - logvar: Matrix representing estimataed variance in log-space (N, Z), with Z latent space dimension\n",
    "        \"\"\"\n",
    "\n",
    "        # Replace \"pass\" statement with your code\n",
    "        encoder = self.encoder(x)\n",
    "        mu      = self.mu_layer(encoder)\n",
    "        logvar  = self.logvar_layer(encoder)\n",
    "        z       = reparametrize(mu, logvar)\n",
    "        x_hat   = self.decoder(z)\n",
    "        \n",
    "\n",
    "        return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719048fc",
   "metadata": {},
   "source": [
    "**CVAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84641daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, input_size, num_classes=10, latent_size=15):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.input_size = input_size # H*W\n",
    "        self.latent_size = latent_size # Z\n",
    "        self.num_classes = num_classes # C\n",
    "        self.hidden_dim = 200 # H_d\n",
    "        self.encoder = Sequential()\n",
    "        self.mu_layer = Dense(self.latent_size)\n",
    "        self.logvar_layer = Dense(self.latent_size)\n",
    "        self.decoder = Sequential()\n",
    "\n",
    "\n",
    "        # Replace \"pass\" statement with your code\n",
    "        # self.encoder.add(Flatten(input_shape = self.input_size))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "\n",
    "\n",
    "        # Replace \"pass\" statement with your code\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.input_size, activation = 'sigmoid'))\n",
    "        self.decoder.add(Reshape((28,28)))\n",
    "\n",
    "\n",
    "    def call(self, x, c):\n",
    "        \"\"\"\n",
    "        Performs forward pass through FC-CVAE model by passing image through \n",
    "        encoder, reparametrize trick, and decoder models\n",
    "    \n",
    "        Inputs:\n",
    "        - x: Input data for this timestep of shape (N, 1, H, W)\n",
    "        - c: One hot vector representing the input class (0-9) (N, C)\n",
    "        \n",
    "        Returns:\n",
    "        - x_hat: Reconstruced input data of shape (N, 1, H, W)\n",
    "        - mu: Matrix representing estimated posterior mu (N, Z), with Z latent space dimension\n",
    "        - logvar: Matrix representing estimated variance in log-space (N, Z),  with Z latent space dimension\n",
    "        \"\"\"\n",
    "\n",
    "        # Replace \"pass\" statement with your code\n",
    "        x_flat    = Flatten(input_shape = (28,28))\n",
    "        initial_1 = tf.concat([x_flat(x),tf.cast(c, dtype = \"float32\")],1)          #tf.concat([x_flat(x), tf.reshape(tf.cast(c, dtype = \"float32\"),(-1,1))], 1)\n",
    "        encoder   = self.encoder(initial_1)\n",
    "        mu        = self.mu_layer(encoder)\n",
    "        logvar    = self.logvar_layer(encoder)\n",
    "        z         = reparametrize(mu, logvar)\n",
    "        initial_2 = tf.concat([z,tf.cast(c, dtype = \"float32\")],1) \n",
    "        x_hat     = self.decoder(initial_2)\n",
    "\n",
    "        return x_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9b7f7",
   "metadata": {},
   "source": [
    "**Tool Function** Checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "66a791df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar): # Checked\n",
    "    \"\"\"\n",
    "    Differentiably sample random Gaussian data with specified mean and variance using the\n",
    "    reparameterization trick.\n",
    "\n",
    "    Suppose we want to sample a random number z from a Gaussian distribution with mean mu and\n",
    "    standard deviation sigma, such that we can backpropagate from the z back to mu and sigma.\n",
    "    We can achieve this by first sampling a random value epsilon from a standard Gaussian\n",
    "    distribution with zero mean and unit variance, then setting z = sigma * epsilon + mu.\n",
    "\n",
    "    For more stable training when integrating this function into a neural network, it helps to\n",
    "    pass this function the log of the variance of the distribution from which to sample, rather\n",
    "    than specifying the standard deviation directly.\n",
    "\n",
    "    Inputs:\n",
    "    - mu: Tensor of shape (N, Z) giving means\n",
    "    - logvar: Tensor of shape (N, Z) giving log-variances\n",
    "\n",
    "    Returns: \n",
    "    - z: Estimated latent vectors, where z[i, j] is a random value sampled from a Gaussian with\n",
    "         mean mu[i, j] and log-variance logvar[i, j].\n",
    "    \"\"\"\n",
    "\n",
    "    \"Sample from the normal distribution\"\n",
    "    epsilon = tf.random.normal(shape = tf.shape(mu))\n",
    "    z = mu + tf.exp(0.5 * logvar) * epsilon\n",
    "\n",
    "    return z\n",
    "\n",
    "def bce_function(x_hat, x): # Checked\n",
    "    \"\"\"\n",
    "    Computes the reconstruction loss of the VAE.\n",
    "    \n",
    "    Inputs:\n",
    "    - x_hat: Reconstructed input data of shape (N, 1, H, W)\n",
    "    - x: Input data for this timestep of shape (N, 1, H, W)\n",
    "    \n",
    "    Returns:\n",
    "    - reconstruction_loss: Tensor containing the scalar loss for the reconstruction loss term.\n",
    "    \"\"\"\n",
    "    bce_fn = tf.keras.losses.BinaryCrossentropy(\n",
    "        from_logits=False, \n",
    "        reduction=tf.keras.losses.Reduction.SUM,\n",
    "    )\n",
    "    reconstruction_loss = bce_fn(x, x_hat) * x.shape[-1]  # Sum over all loss terms for each data point. This looks weird, but we need this to work...\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def loss_function(x_hat, x, mu, logvar): # checked\n",
    "    \"\"\"\n",
    "    Computes the negative variational lower bound loss term of the VAE (refer to formulation in notebook).\n",
    "    Returned loss is the average loss per sample in the current batch.\n",
    "\n",
    "    Inputs:\n",
    "    - x_hat: Reconstructed input data of shape (N, 1, H, W)\n",
    "    - x: Input data for this timestep of shape (N, 1, H, W)\n",
    "    - mu: Matrix representing estimated posterior mu (N, Z), with Z latent space dimension\n",
    "    - logvar: Matrix representing estimated variance in log-space (N, Z), with Z latent space dimension\n",
    "    \n",
    "    Returns:\n",
    "    - loss: Tensor containing the scalar loss for the negative variational lowerbound\n",
    "    \"\"\"\n",
    "    reconstruction_loss = bce_function(x_hat, x)\n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar), axis=-1)\n",
    "    loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66992a95",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec233a8",
   "metadata": {},
   "source": [
    "**Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4124ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import sigmoid\n",
    "from tqdm import tqdm\n",
    "from vae import VAE, CVAE, reparametrize, loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56cce0",
   "metadata": {},
   "source": [
    "**parseArguments**  \n",
    "\n",
    "\n",
    "all set of arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0566199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseArguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--is_cvae\", action=\"store_true\")\n",
    "    parser.add_argument(\"--load_weights\", action=\"store_true\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--latent_size\", type=int, default=15)\n",
    "    parser.add_argument(\"--input_size\", type=int, default=28*28)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d72b5",
   "metadata": {},
   "source": [
    "**one_hot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17803fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, class_size):\n",
    "    \"\"\"\n",
    "    Create one hot label matrix of size (N, C)\n",
    "\n",
    "    Inputs:\n",
    "    - labels: Labels Tensor of shape (N,) representing a ground-truth label\n",
    "    for each MNIST image\n",
    "    - class_size: Scalar representing of target classes our dataset \n",
    "    Returns:\n",
    "    - targets: One-hot label matrix of (N, C), where targets[i, j] = 1 when \n",
    "    the ground truth label for image i is j, and targets[i, :j] & \n",
    "    targets[i, j + 1:] are equal to 0\n",
    "    \"\"\"\n",
    "    targets = np.zeros((labels.shape[0], class_size))\n",
    "    for i, label in enumerate(labels):\n",
    "        targets[i, label] = 1\n",
    "    targets = tf.convert_to_tensor(targets)\n",
    "    targets = tf.cast(targets, tf.float32)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47baeef",
   "metadata": {},
   "source": [
    "**train_vae**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4bd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, train_loader, args, is_cvae=False):\n",
    "    \"\"\"\n",
    "    Train your VAE with one epoch.\n",
    "\n",
    "    Inputs:\n",
    "    - model: Your VAE instance.\n",
    "    - train_loader: A tf.data.Dataset of MNIST dataset.\n",
    "    - args: All arguments.\n",
    "    - is_cvae: A boolean flag for Conditional-VAE. If your model is a Conditional-VAE,\n",
    "    set is_cvae=True. If it's a Vanilla-VAE, set is_cvae=False.\n",
    "\n",
    "    Returns:\n",
    "    - total_loss: Sum of loss values of all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    sum_loss = 0\n",
    "    if is_cvae == False:\n",
    "        for batch in train_loader:\n",
    "            x = batch[0]\n",
    "            with tf.GradientTape() as tape:\n",
    "                x_hat, mu, logvar = model.call(x)\n",
    "                loss = loss_function(x_hat, x, mu, logvar)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            sum_loss += tf.reduce_sum(loss)\n",
    "    else:\n",
    "        for batch in train_loader:\n",
    "            x = batch[0]\n",
    "            c = one_hot(batch[1],10)\n",
    "            with tf.GradientTape() as tape:\n",
    "                x_hat, mu, logvar = model.call(x,c)\n",
    "                loss = loss_function(x_hat, x, mu, logvar)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            sum_loss += tf.reduce_sum(loss)\n",
    "        \n",
    "        \n",
    "    return sum_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e31c95",
   "metadata": {},
   "source": [
    "**Load Mnist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661fd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_mnist(batch_size, buffer_size=1024):\n",
    "    \"\"\"\n",
    "    Load and preprocess MNIST dataset from tf.keras.datasets.mnist.\n",
    "\n",
    "    Inputs:\n",
    "    - batch_size: An integer value of batch size.\n",
    "    - buffer_size: Buffer size for random sampling in tf.data.Dataset.shuffle().\n",
    "\n",
    "    Returns:\n",
    "    - train_dataset: A tf.data.Dataset instance of MNIST dataset. Batching and shuffling are already supported.\n",
    "    \"\"\"\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), _ = mnist.load_data()\n",
    "    x_train = x_train / 255.0\n",
    "    x_train = np.expand_dims(x_train, axis=1)  # [batch_sz, channel_sz, height, width]\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=buffer_size).batch(batch_size, drop_remainder=True)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8d747",
   "metadata": {},
   "source": [
    "**Save model weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f47e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model, args):\n",
    "        \"\"\"\n",
    "        Save trained VAE model weights to model_ckpts/\n",
    "\n",
    "        Inputs:\n",
    "        - model: Trained VAE model.\n",
    "        - args: All arguments.\n",
    "        \"\"\"\n",
    "        model_flag = \"cvae\" if args.is_cvae else \"vae\"\n",
    "        output_dir = os.path.join(\"model_ckpts\", model_flag)\n",
    "        output_path = os.path.join(output_dir, model_flag)\n",
    "        os.makedirs(\"model_ckpts\", exist_ok=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model.save_weights(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a878680",
   "metadata": {},
   "source": [
    "**show_vae_images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44eecf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_vae_images(model, latent_size):\n",
    "    \"\"\"\n",
    "    Call this only if the model is VAE!\n",
    "    Generate 10 images from random vectors.\n",
    "    Show the generated images from your trained VAE.\n",
    "    Image will be saved to outputs/show_vae_images.pdf\n",
    "\n",
    "    Inputs:\n",
    "    - model: Your trained model.\n",
    "    - latent_size: Latent size of your model.\n",
    "    \"\"\"\n",
    "    # Generated images from vectors of random values.\n",
    "    z = tf.random.normal(shape=[10, latent_size])\n",
    "    samples = model.decoder(z).numpy()\n",
    "\n",
    "    # Visualize\n",
    "    fig = plt.figure(figsize=(10, 1))\n",
    "    gspec = gridspec.GridSpec(1, 10)\n",
    "    gspec.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gspec[i])\n",
    "        plt.axis(\"off\")\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect(\"equal\")\n",
    "        plt.imshow(sample.reshape(28, 28), cmap=\"Greys_r\")\n",
    "\n",
    "    # Save the generated images\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    output_path = os.path.join(\"outputs\", \"show_vae_images.pdf\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f4a16",
   "metadata": {},
   "source": [
    "**show_vae_interpolation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28b9e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_vae_interpolation(model, latent_size):\n",
    "    \"\"\"\n",
    "    Call this only if the model is VAE!\n",
    "    Generate interpolation between two .\n",
    "    Show the generated images from your trained VAE.\n",
    "    Image will be saved to outputs/show_vae_interpolation.pdf\n",
    "\n",
    "    Inputs:\n",
    "    - model: Your trained model.\n",
    "    - latent_size: Latent size of your model.\n",
    "    \"\"\"\n",
    "    def show_interpolation(images):\n",
    "        \"\"\"\n",
    "        A helper to visualize the interpolation.\n",
    "        \"\"\"\n",
    "        images = tf.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "        sqrtn = int(math.ceil(math.sqrt(images.shape[0])))\n",
    "        sqrtimg = int(math.ceil(math.sqrt(images.shape[1])))\n",
    "\n",
    "        fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "        gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "        for i, img in enumerate(images):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(tf.reshape(img,[sqrtimg,sqrtimg]))\n",
    "\n",
    "        # Save the generated images\n",
    "        os.makedirs(\"outputs\", exist_ok=True)\n",
    "        output_path = os.path.join(\"outputs\", \"show_vae_interpolation.pdf\")\n",
    "        plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    S = 12\n",
    "    z0 = tf.random.normal(shape=[S,latent_size], dtype=tf.dtypes.float32)  # [S, latent_size]\n",
    "    z1 = tf.random.normal(shape=[S,latent_size], dtype=tf.dtypes.float32)\n",
    "    w = tf.linspace(0, 1, S)\n",
    "    w = tf.cast(tf.reshape(w, (S,1,1)), dtype=tf.float32)  # [S, 1, 1]\n",
    "    z = tf.transpose(w * z0 + (1 - w) * z1, perm=[1,0,2])\n",
    "    z = tf.reshape(z, (S*S, latent_size))  # [S, S, latent_size]\n",
    "    x = model.decoder(z)  # [S*S, 1, 28, 28]\n",
    "    show_interpolation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e58ae",
   "metadata": {},
   "source": [
    "**show_cvae_images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef6448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cvae_images(model, latent_size):\n",
    "    \"\"\"\n",
    "    Call this only if the model is CVAE!\n",
    "    Conditionally generate 10 images for each digit.\n",
    "    Show the generated images from your trained CVAE.\n",
    "    Image will be saved to outputs/show_cvae_images.pdf\n",
    "\n",
    "    Inputs:\n",
    "    - model: Your trained model.\n",
    "    - latent_size: Latent size of your model.\n",
    "    \"\"\"\n",
    "    # Conditionally generated images from vectors of random values.\n",
    "    num_generation = 100\n",
    "    num_classes = 10\n",
    "    num_per_class = num_generation // num_classes\n",
    "    c = tf.eye(num_classes) # [one hot labels for 0-9]\n",
    "    z = []\n",
    "    labels = []\n",
    "    for label in range(num_classes):\n",
    "        curr_c = c[label]\n",
    "        curr_c = tf.broadcast_to(curr_c, [num_per_class, len(curr_c)])\n",
    "        curr_z = tf.random.normal(shape=[num_per_class,latent_size])\n",
    "        curr_z = tf.concat([curr_z,curr_c], axis=-1)\n",
    "        z.append(curr_z)\n",
    "        labels.append([label]*num_per_class)\n",
    "    z = np.concatenate(z)\n",
    "    labels = np.concatenate(labels)\n",
    "    samples = model.decoder(z).numpy()\n",
    "\n",
    "    # Visualize\n",
    "    rows = num_classes\n",
    "    cols = num_generation // rows\n",
    "\n",
    "    fig = plt.figure(figsize=(cols, rows))\n",
    "    gspec = gridspec.GridSpec(rows, cols)\n",
    "    gspec.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gspec[i])\n",
    "        plt.axis(\"off\")\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect(\"equal\")\n",
    "        plt.imshow(sample.reshape(28, 28), cmap=\"Greys_r\")\n",
    "\n",
    "    # Save the generated images\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    output_path = os.path.join(\"outputs\", \"show_cvae_images.pdf\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c5bf1",
   "metadata": {},
   "source": [
    "**load_weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2cbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model):\n",
    "    \"\"\"\n",
    "    Load the trained model's weights.\n",
    "\n",
    "    Inputs:\n",
    "    - model: Your untrained model instance.\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained model.\n",
    "    \"\"\"\n",
    "    num_classes = 10\n",
    "    inputs = tf.zeros([1,1,28,28])  # Random data sample\n",
    "    labels = tf.constant([[0]])\n",
    "    if args.is_cvae:\n",
    "        weights_path = os.path.join(\"model_ckpts\", \"cvae\", \"cvae\")\n",
    "        one_hot_vec = one_hot(labels, num_classes)\n",
    "        _ = model(inputs, one_hot_vec)\n",
    "        model.load_weights(weights_path)\n",
    "    else:\n",
    "        weights_path = os.path.join(\"model_ckpts\", \"vae\", \"vae\")\n",
    "        _ = model(inputs)\n",
    "        model.load_weights(weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d821e1",
   "metadata": {},
   "source": [
    "**main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09785210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Load MNIST dataset\n",
    "    train_dataset = load_mnist(args.batch_size)\n",
    "\n",
    "    # Get an instance of VAE\n",
    "    if args.is_cvae:\n",
    "        model = CVAE(args.input_size, latent_size=args.latent_size)\n",
    "    else:\n",
    "        model = VAE(args.input_size, latent_size=args.latent_size)\n",
    "\n",
    "    # Load trained weights\n",
    "    #if args.load_weights:\n",
    "    #    model = load_weights(model)\n",
    "\n",
    "    # Train VAE\n",
    "    for epoch_id in range(args.num_epochs):\n",
    "        total_loss = train_vae(model, train_dataset, args, is_cvae=args.is_cvae)\n",
    "        print(f\"Train Epoch: {epoch_id} \\tLoss: {total_loss/len(train_dataset):.6f}\")\n",
    "\n",
    "    # Visualize results\n",
    "    if args.is_cvae:\n",
    "        show_cvae_images(model, args.latent_size)\n",
    "    else:\n",
    "        show_vae_images(model, args.latent_size)\n",
    "        show_vae_interpolation(model, args.latent_size)\n",
    "\n",
    "    # Optional: Save VAE/CVAE model for debugging/testing.\n",
    "    save_model_weights(model, args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parseArguments()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b7e60",
   "metadata": {},
   "source": [
    "## =========================== Free try ====================================\n",
    "## ======================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a340c7",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "73bbec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.compat.v1.data.make_one_shot_iterator(load_mnist(128, buffer_size=1024)).get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a87711a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = dataset[0]\n",
    "cc = dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e060a",
   "metadata": {},
   "source": [
    "## Checking Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "be8712d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, input_size, latent_size=15):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_size  = input_size     # H*W  Original dimention\n",
    "        self.latent_size = latent_size   # Z \n",
    "        self.hidden_dim  = 400            # H_d \n",
    "\n",
    "        \n",
    "        \n",
    "        self.encoder      = Sequential()\n",
    "        self.decoder      = Sequential()\n",
    "        self.mu_layer     = Dense(self.latent_size)\n",
    "        self.logvar_layer = Dense(self.latent_size)\n",
    "        self.optimizer    = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder.add(Flatten(input_shape = (1,28,28))) # input_shape = self.input_size (int(sqrt(self.input_size)),int(sqrt(self.input_size)))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.encoder.add(Dense(units = self.hidden_dim, activation = 'relu')) # add later\n",
    "        \n",
    "\n",
    "        # Decoder\n",
    "        self.decoder.add(tf.keras.layers.Input(self.latent_size, ))\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.hidden_dim, activation = 'relu'))\n",
    "        self.decoder.add(Dense(units = self.input_size, activation = 'sigmoid'))\n",
    "        self.decoder.add(Reshape((1,28,28)))\n",
    "        \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through FC-VAE model by passing image through \n",
    "        encoder, reparametrize trick, and decoder models\n",
    "    \n",
    "        Inputs:\n",
    "        - x: Batch of input images of shape (N, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "        - x_hat: Reconstruced input data of shape (N,1,H,W)\n",
    "        - mu: Matrix representing estimated posterior mu (N, Z), with Z latent space dimension\n",
    "        - logvar: Matrix representing estimataed variance in log-space (N, Z), with Z latent space dimension\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward\n",
    "        encoder_out = self.encoder(x)\n",
    "        mu          = self.mu_layer(encoder_out)\n",
    "        logvar      = self.logvar_layer(encoder_out)\n",
    "        z           = reparametrize(mu, logvar)\n",
    "        x_hat       = self.decoder(z)\n",
    "        \n",
    "\n",
    "        return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ecb8d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, train_loader, args, is_cvae=False):\n",
    "    \"\"\"\n",
    "    Train your VAE with one epoch.\n",
    "\n",
    "    Inputs:\n",
    "    - model: Your VAE instance.\n",
    "    - train_loader: A tf.data.Dataset of MNIST dataset.\n",
    "    - args: All arguments.\n",
    "    - is_cvae: A boolean flag for Conditional-VAE. If your model is a Conditional-VAE,\n",
    "    set is_cvae=True. If it's a Vanilla-VAE, set is_cvae=False.\n",
    "\n",
    "    Returns:\n",
    "    - total_loss: Sum of loss values of all batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    sum_loss = 0\n",
    "    itr = 0\n",
    "    if is_cvae == False:\n",
    "        for batch in train_loader:\n",
    "            x = batch[0]\n",
    "            with tf.GradientTape() as tape:\n",
    "                x_hat, mu, logvar = model.call(x)\n",
    "                loss = loss_function(x_hat, x, mu, logvar)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            # itr += 1\n",
    "            # print(f\"{itr}th step: The loss is [{tf.round(loss,2)}] x size {x.shape}\")\n",
    "            sum_loss += loss\n",
    "    else:\n",
    "        for batch in train_loader:\n",
    "            x = batch[0]\n",
    "            c = one_hot(batch[1],10)\n",
    "            with tf.GradientTape() as tape:\n",
    "                x_hat, mu, logvar = model.call(x,c)\n",
    "                loss = loss_function(x_hat, x, mu, logvar)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            # print(f\"The loss is {loss} c shape {c.shape}\")\n",
    "            sum_loss += loss\n",
    "        \n",
    "        \n",
    "    return sum_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d11d4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the negative variational lower bound loss term of the VAE (refer to formulation in notebook).\n",
    "    Returned loss is the average loss per sample in the current batch.\n",
    "\n",
    "    Inputs:\n",
    "    - x_hat: Reconstructed input data of shape (N, 1, H, W)\n",
    "    - x: Input data for this timestep of shape (N, 1, H, W)\n",
    "    - mu: Matrix representing estimated posterior mu (N, Z), with Z latent space dimension\n",
    "    - logvar: Matrix representing estimated variance in log-space (N, Z), with Z latent space dimension\n",
    "    \n",
    "    Returns:\n",
    "    - loss: Tensor containing the scalar loss for the negative variational lowerbound\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    ################################################################################################\n",
    "    # TODO: Compute negative variational lowerbound loss as described in the notebook              #\n",
    "    ################################################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    loss = bce_function(x_hat, x) / x.shape[0]\n",
    "    loss += -0.5 * tf.reduce_mean(tf.reduce_sum(1 + logvar - square(mu) - exp(logvar), axis=-1))\n",
    "    \n",
    "    ################################################################################################\n",
    "    #                            END OF YOUR CODE                                                  #\n",
    "    ################################################################################################\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1db4f",
   "metadata": {},
   "source": [
    "## Input Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbddd3",
   "metadata": {},
   "source": [
    "## VA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "493fbe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "1th iteration ===========================================================================\n",
      "iteration 1: average loss is 179.89028930664062\n",
      "2th iteration ===========================================================================\n",
      "iteration 2: average loss is 131.95040893554688\n",
      "3th iteration ===========================================================================\n",
      "iteration 3: average loss is 122.48843383789062\n",
      "4th iteration ===========================================================================\n",
      "iteration 4: average loss is 117.58981323242188\n",
      "5th iteration ===========================================================================\n",
      "iteration 5: average loss is 114.05528259277344\n",
      "6th iteration ===========================================================================\n",
      "iteration 6: average loss is 111.8182601928711\n",
      "7th iteration ===========================================================================\n",
      "iteration 7: average loss is 110.28640747070312\n",
      "8th iteration ===========================================================================\n",
      "iteration 8: average loss is 109.09989166259766\n",
      "9th iteration ===========================================================================\n",
      "iteration 9: average loss is 108.18840026855469\n",
      "10th iteration ===========================================================================\n",
      "iteration 10: average loss is 107.3702163696289\n"
     ]
    }
   ],
   "source": [
    "VA = VAE(784)\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}th iteration ===========================================================================\")\n",
    "    a = train_vae(VA,load_mnist(128, buffer_size=1024) , 1, is_cvae=False)\n",
    "    print(f\"iteration {i+1}: average loss is {a/len(load_mnist(128, buffer_size=1024))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18038d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(VA.call(xx)[0], xx, VA.call(xx)[1], VA.call(xx)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09d89c",
   "metadata": {},
   "source": [
    "## CVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7799635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration ===========================================================================\n",
      "The loss is 69642.421875 c shape (128, 10)\n",
      "The loss is 68826.734375 c shape (128, 10)\n",
      "The loss is 67871.90625 c shape (128, 10)\n",
      "The loss is 66320.21875 c shape (128, 10)\n",
      "The loss is 63671.09375 c shape (128, 10)\n",
      "The loss is 59998.08203125 c shape (128, 10)\n",
      "The loss is 54021.578125 c shape (128, 10)\n",
      "The loss is 47801.0390625 c shape (128, 10)\n",
      "The loss is 42701.38671875 c shape (128, 10)\n",
      "The loss is 41899.6796875 c shape (128, 10)\n",
      "The loss is 42053.65234375 c shape (128, 10)\n",
      "The loss is 36781.51171875 c shape (128, 10)\n",
      "The loss is 33373.03515625 c shape (128, 10)\n",
      "The loss is 30556.658203125 c shape (128, 10)\n",
      "The loss is 30599.41015625 c shape (128, 10)\n",
      "The loss is 30897.09375 c shape (128, 10)\n",
      "The loss is 30025.287109375 c shape (128, 10)\n",
      "The loss is 29365.80078125 c shape (128, 10)\n",
      "The loss is 28809.822265625 c shape (128, 10)\n",
      "The loss is 28110.572265625 c shape (128, 10)\n",
      "The loss is 28969.12890625 c shape (128, 10)\n",
      "The loss is 27842.0390625 c shape (128, 10)\n",
      "The loss is 28756.154296875 c shape (128, 10)\n",
      "The loss is 28255.166015625 c shape (128, 10)\n",
      "The loss is 27157.498046875 c shape (128, 10)\n",
      "The loss is 26465.18359375 c shape (128, 10)\n",
      "The loss is 26756.9140625 c shape (128, 10)\n",
      "The loss is 26770.8984375 c shape (128, 10)\n",
      "The loss is 27612.998046875 c shape (128, 10)\n",
      "The loss is 27380.345703125 c shape (128, 10)\n",
      "The loss is 27497.48046875 c shape (128, 10)\n",
      "The loss is 26641.35546875 c shape (128, 10)\n",
      "The loss is 27140.638671875 c shape (128, 10)\n",
      "The loss is 26902.111328125 c shape (128, 10)\n",
      "The loss is 28118.33203125 c shape (128, 10)\n",
      "The loss is 27016.197265625 c shape (128, 10)\n",
      "The loss is 25677.20703125 c shape (128, 10)\n",
      "The loss is 27280.466796875 c shape (128, 10)\n",
      "The loss is 27240.794921875 c shape (128, 10)\n",
      "The loss is 26404.0078125 c shape (128, 10)\n",
      "The loss is 26810.27734375 c shape (128, 10)\n",
      "The loss is 27496.154296875 c shape (128, 10)\n",
      "The loss is 25593.3125 c shape (128, 10)\n",
      "The loss is 27234.609375 c shape (128, 10)\n",
      "The loss is 25817.31640625 c shape (128, 10)\n",
      "The loss is 25205.802734375 c shape (128, 10)\n",
      "The loss is 25244.060546875 c shape (128, 10)\n",
      "The loss is 26123.890625 c shape (128, 10)\n",
      "The loss is 25241.4296875 c shape (128, 10)\n",
      "The loss is 25488.416015625 c shape (128, 10)\n",
      "The loss is 24724.296875 c shape (128, 10)\n",
      "The loss is 24884.8984375 c shape (128, 10)\n",
      "The loss is 25757.80859375 c shape (128, 10)\n",
      "The loss is 24994.05078125 c shape (128, 10)\n",
      "The loss is 25716.8203125 c shape (128, 10)\n",
      "The loss is 24810.82421875 c shape (128, 10)\n",
      "The loss is 25563.244140625 c shape (128, 10)\n",
      "The loss is 24876.455078125 c shape (128, 10)\n",
      "The loss is 25997.8203125 c shape (128, 10)\n",
      "The loss is 25304.21484375 c shape (128, 10)\n",
      "The loss is 24456.73046875 c shape (128, 10)\n",
      "The loss is 24038.6875 c shape (128, 10)\n",
      "The loss is 26044.02734375 c shape (128, 10)\n",
      "The loss is 26204.8046875 c shape (128, 10)\n",
      "The loss is 25095.615234375 c shape (128, 10)\n",
      "The loss is 25830.5 c shape (128, 10)\n",
      "The loss is 25935.494140625 c shape (128, 10)\n",
      "The loss is 24641.53515625 c shape (128, 10)\n",
      "The loss is 25139.330078125 c shape (128, 10)\n",
      "The loss is 24836.357421875 c shape (128, 10)\n",
      "The loss is 25247.99609375 c shape (128, 10)\n",
      "The loss is 26170.099609375 c shape (128, 10)\n",
      "The loss is 25970.85546875 c shape (128, 10)\n",
      "The loss is 27225.560546875 c shape (128, 10)\n",
      "The loss is 25749.994140625 c shape (128, 10)\n",
      "The loss is 25365.134765625 c shape (128, 10)\n",
      "The loss is 24972.90625 c shape (128, 10)\n",
      "The loss is 25105.82421875 c shape (128, 10)\n",
      "The loss is 23985.83203125 c shape (128, 10)\n",
      "The loss is 25397.353515625 c shape (128, 10)\n",
      "The loss is 23566.0546875 c shape (128, 10)\n",
      "The loss is 25268.91796875 c shape (128, 10)\n",
      "The loss is 24267.427734375 c shape (128, 10)\n",
      "The loss is 23978.96484375 c shape (128, 10)\n",
      "The loss is 23535.265625 c shape (128, 10)\n",
      "The loss is 24187.2734375 c shape (128, 10)\n",
      "The loss is 24599.8046875 c shape (128, 10)\n",
      "The loss is 24088.953125 c shape (128, 10)\n",
      "The loss is 24684.05078125 c shape (128, 10)\n",
      "The loss is 24194.39453125 c shape (128, 10)\n",
      "The loss is 24976.81640625 c shape (128, 10)\n",
      "The loss is 24687.150390625 c shape (128, 10)\n",
      "The loss is 23698.234375 c shape (128, 10)\n",
      "The loss is 23251.13671875 c shape (128, 10)\n",
      "The loss is 24890.44921875 c shape (128, 10)\n",
      "The loss is 24259.40234375 c shape (128, 10)\n",
      "The loss is 25202.181640625 c shape (128, 10)\n",
      "The loss is 24701.736328125 c shape (128, 10)\n",
      "The loss is 24884.583984375 c shape (128, 10)\n",
      "The loss is 24152.546875 c shape (128, 10)\n",
      "The loss is 22985.509765625 c shape (128, 10)\n",
      "The loss is 24475.669921875 c shape (128, 10)\n",
      "The loss is 24123.18359375 c shape (128, 10)\n",
      "The loss is 25300.73828125 c shape (128, 10)\n",
      "The loss is 24967.724609375 c shape (128, 10)\n",
      "The loss is 24085.740234375 c shape (128, 10)\n",
      "The loss is 23784.490234375 c shape (128, 10)\n",
      "The loss is 24177.57421875 c shape (128, 10)\n",
      "The loss is 24338.56640625 c shape (128, 10)\n",
      "The loss is 23668.13671875 c shape (128, 10)\n",
      "The loss is 24587.16796875 c shape (128, 10)\n",
      "The loss is 23177.7890625 c shape (128, 10)\n",
      "The loss is 22981.123046875 c shape (128, 10)\n",
      "The loss is 22599.47265625 c shape (128, 10)\n",
      "The loss is 23364.87890625 c shape (128, 10)\n",
      "The loss is 22541.28125 c shape (128, 10)\n",
      "The loss is 23578.1875 c shape (128, 10)\n",
      "The loss is 23336.86328125 c shape (128, 10)\n",
      "The loss is 22832.93359375 c shape (128, 10)\n",
      "The loss is 23294.65234375 c shape (128, 10)\n",
      "The loss is 22724.828125 c shape (128, 10)\n",
      "The loss is 22552.408203125 c shape (128, 10)\n",
      "The loss is 22674.662109375 c shape (128, 10)\n",
      "The loss is 21491.30859375 c shape (128, 10)\n",
      "The loss is 22944.3359375 c shape (128, 10)\n",
      "The loss is 21936.84375 c shape (128, 10)\n",
      "The loss is 21773.44921875 c shape (128, 10)\n",
      "The loss is 22680.45703125 c shape (128, 10)\n",
      "The loss is 23014.494140625 c shape (128, 10)\n",
      "The loss is 22304.171875 c shape (128, 10)\n",
      "The loss is 21800.0078125 c shape (128, 10)\n",
      "The loss is 21439.787109375 c shape (128, 10)\n",
      "The loss is 21707.478515625 c shape (128, 10)\n",
      "The loss is 21123.3671875 c shape (128, 10)\n",
      "The loss is 20607.525390625 c shape (128, 10)\n",
      "The loss is 22248.52734375 c shape (128, 10)\n",
      "The loss is 21213.759765625 c shape (128, 10)\n",
      "The loss is 20761.033203125 c shape (128, 10)\n",
      "The loss is 21424.986328125 c shape (128, 10)\n",
      "The loss is 20670.80078125 c shape (128, 10)\n",
      "The loss is 20262.091796875 c shape (128, 10)\n",
      "The loss is 20276.552734375 c shape (128, 10)\n",
      "The loss is 21784.837890625 c shape (128, 10)\n",
      "The loss is 20457.10546875 c shape (128, 10)\n",
      "The loss is 19930.21484375 c shape (128, 10)\n",
      "The loss is 21593.33203125 c shape (128, 10)\n",
      "The loss is 20091.3046875 c shape (128, 10)\n",
      "The loss is 19811.05078125 c shape (128, 10)\n",
      "The loss is 20316.99609375 c shape (128, 10)\n",
      "The loss is 21814.18359375 c shape (128, 10)\n",
      "The loss is 20553.658203125 c shape (128, 10)\n",
      "The loss is 20242.68359375 c shape (128, 10)\n",
      "The loss is 20085.703125 c shape (128, 10)\n",
      "The loss is 20209.9140625 c shape (128, 10)\n",
      "The loss is 18375.58203125 c shape (128, 10)\n",
      "The loss is 20368.6796875 c shape (128, 10)\n",
      "The loss is 19813.37890625 c shape (128, 10)\n",
      "The loss is 18978.47265625 c shape (128, 10)\n",
      "The loss is 19140.58203125 c shape (128, 10)\n",
      "The loss is 19927.732421875 c shape (128, 10)\n",
      "The loss is 19664.779296875 c shape (128, 10)\n",
      "The loss is 19830.453125 c shape (128, 10)\n",
      "The loss is 18937.728515625 c shape (128, 10)\n",
      "The loss is 19843.28125 c shape (128, 10)\n",
      "The loss is 19085.375 c shape (128, 10)\n",
      "The loss is 19948.73828125 c shape (128, 10)\n",
      "The loss is 19149.03125 c shape (128, 10)\n",
      "The loss is 18782.6328125 c shape (128, 10)\n",
      "The loss is 19174.6171875 c shape (128, 10)\n",
      "The loss is 19209.0390625 c shape (128, 10)\n",
      "The loss is 18852.740234375 c shape (128, 10)\n",
      "The loss is 19240.8203125 c shape (128, 10)\n",
      "The loss is 18601.783203125 c shape (128, 10)\n",
      "The loss is 20097.671875 c shape (128, 10)\n",
      "The loss is 19575.328125 c shape (128, 10)\n",
      "The loss is 18889.916015625 c shape (128, 10)\n",
      "The loss is 18553.9765625 c shape (128, 10)\n",
      "The loss is 18306.9453125 c shape (128, 10)\n",
      "The loss is 19273.26953125 c shape (128, 10)\n",
      "The loss is 19183.734375 c shape (128, 10)\n",
      "The loss is 18685.203125 c shape (128, 10)\n",
      "The loss is 18126.087890625 c shape (128, 10)\n",
      "The loss is 18220.982421875 c shape (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 18290.548828125 c shape (128, 10)\n",
      "The loss is 17296.015625 c shape (128, 10)\n",
      "The loss is 18530.08203125 c shape (128, 10)\n",
      "The loss is 18809.6875 c shape (128, 10)\n",
      "The loss is 18287.359375 c shape (128, 10)\n",
      "The loss is 18228.955078125 c shape (128, 10)\n",
      "The loss is 17800.666015625 c shape (128, 10)\n",
      "The loss is 17207.41015625 c shape (128, 10)\n",
      "The loss is 18376.828125 c shape (128, 10)\n",
      "The loss is 17734.37109375 c shape (128, 10)\n",
      "The loss is 17540.408203125 c shape (128, 10)\n",
      "The loss is 17070.23046875 c shape (128, 10)\n",
      "The loss is 17256.30859375 c shape (128, 10)\n",
      "The loss is 16878.953125 c shape (128, 10)\n",
      "The loss is 17738.27734375 c shape (128, 10)\n",
      "The loss is 17581.123046875 c shape (128, 10)\n",
      "The loss is 17243.568359375 c shape (128, 10)\n",
      "The loss is 17849.619140625 c shape (128, 10)\n",
      "The loss is 17625.111328125 c shape (128, 10)\n",
      "The loss is 17516.328125 c shape (128, 10)\n",
      "The loss is 16517.068359375 c shape (128, 10)\n",
      "The loss is 16954.998046875 c shape (128, 10)\n",
      "The loss is 17668.59765625 c shape (128, 10)\n",
      "The loss is 18443.390625 c shape (128, 10)\n",
      "The loss is 18018.8359375 c shape (128, 10)\n",
      "The loss is 17832.05859375 c shape (128, 10)\n",
      "The loss is 17229.3125 c shape (128, 10)\n",
      "The loss is 17196.359375 c shape (128, 10)\n",
      "The loss is 18039.330078125 c shape (128, 10)\n",
      "The loss is 17937.796875 c shape (128, 10)\n",
      "The loss is 16520.931640625 c shape (128, 10)\n",
      "The loss is 17145.88671875 c shape (128, 10)\n",
      "The loss is 17872.548828125 c shape (128, 10)\n",
      "The loss is 17347.134765625 c shape (128, 10)\n",
      "The loss is 17129.85546875 c shape (128, 10)\n",
      "The loss is 17775.16796875 c shape (128, 10)\n",
      "The loss is 17577.84375 c shape (128, 10)\n",
      "The loss is 17629.68359375 c shape (128, 10)\n",
      "The loss is 17073.1484375 c shape (128, 10)\n",
      "The loss is 17989.783203125 c shape (128, 10)\n",
      "The loss is 17478.15234375 c shape (128, 10)\n",
      "The loss is 16687.8046875 c shape (128, 10)\n",
      "The loss is 16641.09765625 c shape (128, 10)\n",
      "The loss is 17439.662109375 c shape (128, 10)\n",
      "The loss is 16547.2578125 c shape (128, 10)\n",
      "The loss is 17301.51953125 c shape (128, 10)\n",
      "The loss is 17137.849609375 c shape (128, 10)\n",
      "The loss is 17402.53125 c shape (128, 10)\n",
      "The loss is 16403.240234375 c shape (128, 10)\n",
      "The loss is 17486.8984375 c shape (128, 10)\n",
      "The loss is 17029.0234375 c shape (128, 10)\n",
      "The loss is 17363.078125 c shape (128, 10)\n",
      "The loss is 17104.1484375 c shape (128, 10)\n",
      "The loss is 16808.482421875 c shape (128, 10)\n",
      "The loss is 16974.6484375 c shape (128, 10)\n",
      "The loss is 16766.3359375 c shape (128, 10)\n",
      "The loss is 16982.986328125 c shape (128, 10)\n",
      "The loss is 17027.623046875 c shape (128, 10)\n",
      "The loss is 16911.595703125 c shape (128, 10)\n",
      "The loss is 16041.376953125 c shape (128, 10)\n",
      "The loss is 16683.28515625 c shape (128, 10)\n",
      "The loss is 16667.546875 c shape (128, 10)\n",
      "The loss is 16430.87890625 c shape (128, 10)\n",
      "The loss is 16737.513671875 c shape (128, 10)\n",
      "The loss is 16586.783203125 c shape (128, 10)\n",
      "The loss is 17056.37109375 c shape (128, 10)\n",
      "The loss is 16634.345703125 c shape (128, 10)\n",
      "The loss is 16310.798828125 c shape (128, 10)\n",
      "The loss is 16742.37109375 c shape (128, 10)\n",
      "The loss is 16511.25 c shape (128, 10)\n",
      "The loss is 17092.65625 c shape (128, 10)\n",
      "The loss is 16690.27734375 c shape (128, 10)\n",
      "The loss is 15761.5595703125 c shape (128, 10)\n",
      "The loss is 16036.23828125 c shape (128, 10)\n",
      "The loss is 15686.919921875 c shape (128, 10)\n",
      "The loss is 15599.556640625 c shape (128, 10)\n",
      "The loss is 16257.7294921875 c shape (128, 10)\n",
      "The loss is 16326.6826171875 c shape (128, 10)\n",
      "The loss is 16051.369140625 c shape (128, 10)\n",
      "The loss is 16338.25 c shape (128, 10)\n",
      "The loss is 16427.294921875 c shape (128, 10)\n",
      "The loss is 15802.89453125 c shape (128, 10)\n",
      "The loss is 16339.22265625 c shape (128, 10)\n",
      "The loss is 17492.0546875 c shape (128, 10)\n",
      "The loss is 16710.41015625 c shape (128, 10)\n",
      "The loss is 15925.552734375 c shape (128, 10)\n",
      "The loss is 16204.5634765625 c shape (128, 10)\n",
      "The loss is 15727.275390625 c shape (128, 10)\n",
      "The loss is 16573.34765625 c shape (128, 10)\n",
      "The loss is 15376.97265625 c shape (128, 10)\n",
      "The loss is 16313.748046875 c shape (128, 10)\n",
      "The loss is 16035.8447265625 c shape (128, 10)\n",
      "The loss is 15310.4296875 c shape (128, 10)\n",
      "The loss is 16384.36328125 c shape (128, 10)\n",
      "The loss is 17081.33203125 c shape (128, 10)\n",
      "The loss is 16225.7998046875 c shape (128, 10)\n",
      "The loss is 15969.6669921875 c shape (128, 10)\n",
      "The loss is 16928.57421875 c shape (128, 10)\n",
      "The loss is 15974.37890625 c shape (128, 10)\n",
      "The loss is 15761.5947265625 c shape (128, 10)\n",
      "The loss is 15955.53515625 c shape (128, 10)\n",
      "The loss is 16658.40234375 c shape (128, 10)\n",
      "The loss is 16036.82421875 c shape (128, 10)\n",
      "The loss is 16787.951171875 c shape (128, 10)\n",
      "The loss is 16576.736328125 c shape (128, 10)\n",
      "The loss is 16507.76171875 c shape (128, 10)\n",
      "The loss is 15586.7080078125 c shape (128, 10)\n",
      "The loss is 15822.974609375 c shape (128, 10)\n",
      "The loss is 15832.078125 c shape (128, 10)\n",
      "The loss is 16457.70703125 c shape (128, 10)\n",
      "The loss is 16416.845703125 c shape (128, 10)\n",
      "The loss is 17018.69921875 c shape (128, 10)\n",
      "The loss is 15459.1796875 c shape (128, 10)\n",
      "The loss is 16175.62890625 c shape (128, 10)\n",
      "The loss is 15388.6875 c shape (128, 10)\n",
      "The loss is 15785.974609375 c shape (128, 10)\n",
      "The loss is 15322.3671875 c shape (128, 10)\n",
      "The loss is 16384.14453125 c shape (128, 10)\n",
      "The loss is 16072.41796875 c shape (128, 10)\n",
      "The loss is 14924.2880859375 c shape (128, 10)\n",
      "The loss is 15699.712890625 c shape (128, 10)\n",
      "The loss is 14571.884765625 c shape (128, 10)\n",
      "The loss is 16591.89453125 c shape (128, 10)\n",
      "The loss is 15657.703125 c shape (128, 10)\n",
      "The loss is 15577.87109375 c shape (128, 10)\n",
      "The loss is 14877.140625 c shape (128, 10)\n",
      "The loss is 15458.8828125 c shape (128, 10)\n",
      "The loss is 16145.5458984375 c shape (128, 10)\n",
      "The loss is 15698.1611328125 c shape (128, 10)\n",
      "The loss is 15266.642578125 c shape (128, 10)\n",
      "The loss is 15636.654296875 c shape (128, 10)\n",
      "The loss is 15743.81640625 c shape (128, 10)\n",
      "The loss is 15808.21875 c shape (128, 10)\n",
      "The loss is 15684.357421875 c shape (128, 10)\n",
      "The loss is 15858.30078125 c shape (128, 10)\n",
      "The loss is 15469.685546875 c shape (128, 10)\n",
      "The loss is 14384.6953125 c shape (128, 10)\n",
      "The loss is 15563.142578125 c shape (128, 10)\n",
      "The loss is 14984.373046875 c shape (128, 10)\n",
      "The loss is 15263.0078125 c shape (128, 10)\n",
      "The loss is 15373.109375 c shape (128, 10)\n",
      "The loss is 15586.236328125 c shape (128, 10)\n",
      "The loss is 15227.99609375 c shape (128, 10)\n",
      "The loss is 14879.658203125 c shape (128, 10)\n",
      "The loss is 15855.80859375 c shape (128, 10)\n",
      "The loss is 15112.8056640625 c shape (128, 10)\n",
      "The loss is 15152.923828125 c shape (128, 10)\n",
      "The loss is 15510.7939453125 c shape (128, 10)\n",
      "The loss is 15772.009765625 c shape (128, 10)\n",
      "The loss is 15954.0 c shape (128, 10)\n",
      "The loss is 14523.8544921875 c shape (128, 10)\n",
      "The loss is 15375.0 c shape (128, 10)\n",
      "The loss is 14686.1796875 c shape (128, 10)\n",
      "The loss is 14210.7119140625 c shape (128, 10)\n",
      "The loss is 14934.3935546875 c shape (128, 10)\n",
      "The loss is 14756.140625 c shape (128, 10)\n",
      "The loss is 14741.3203125 c shape (128, 10)\n",
      "The loss is 15592.3759765625 c shape (128, 10)\n",
      "The loss is 14846.783203125 c shape (128, 10)\n",
      "The loss is 15465.5009765625 c shape (128, 10)\n",
      "The loss is 15098.984375 c shape (128, 10)\n",
      "The loss is 15130.552734375 c shape (128, 10)\n",
      "The loss is 15554.1796875 c shape (128, 10)\n",
      "The loss is 15903.306640625 c shape (128, 10)\n",
      "The loss is 14864.119140625 c shape (128, 10)\n",
      "The loss is 15281.767578125 c shape (128, 10)\n",
      "The loss is 14041.41015625 c shape (128, 10)\n",
      "The loss is 15984.0927734375 c shape (128, 10)\n",
      "The loss is 15488.5537109375 c shape (128, 10)\n",
      "The loss is 15043.23828125 c shape (128, 10)\n",
      "The loss is 15410.3818359375 c shape (128, 10)\n",
      "The loss is 15753.732421875 c shape (128, 10)\n",
      "The loss is 15452.236328125 c shape (128, 10)\n",
      "The loss is 15171.46875 c shape (128, 10)\n",
      "The loss is 15397.68359375 c shape (128, 10)\n",
      "The loss is 15407.216796875 c shape (128, 10)\n",
      "The loss is 15388.2529296875 c shape (128, 10)\n",
      "The loss is 14964.7607421875 c shape (128, 10)\n",
      "The loss is 15555.859375 c shape (128, 10)\n",
      "The loss is 15124.7568359375 c shape (128, 10)\n",
      "The loss is 15182.4599609375 c shape (128, 10)\n",
      "The loss is 14995.876953125 c shape (128, 10)\n",
      "The loss is 14674.923828125 c shape (128, 10)\n",
      "The loss is 15174.0107421875 c shape (128, 10)\n",
      "The loss is 15029.5703125 c shape (128, 10)\n",
      "The loss is 14981.4892578125 c shape (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 14798.6298828125 c shape (128, 10)\n",
      "The loss is 15790.7060546875 c shape (128, 10)\n",
      "The loss is 14733.537109375 c shape (128, 10)\n",
      "The loss is 15034.740234375 c shape (128, 10)\n",
      "The loss is 14851.5732421875 c shape (128, 10)\n",
      "The loss is 15013.318359375 c shape (128, 10)\n",
      "The loss is 15260.6015625 c shape (128, 10)\n",
      "The loss is 15211.1767578125 c shape (128, 10)\n",
      "The loss is 14659.796875 c shape (128, 10)\n",
      "The loss is 15612.7119140625 c shape (128, 10)\n",
      "The loss is 15323.0947265625 c shape (128, 10)\n",
      "The loss is 14851.296875 c shape (128, 10)\n",
      "The loss is 15558.552734375 c shape (128, 10)\n",
      "The loss is 14611.3359375 c shape (128, 10)\n",
      "The loss is 15088.59765625 c shape (128, 10)\n",
      "The loss is 15697.77734375 c shape (128, 10)\n",
      "The loss is 14075.458984375 c shape (128, 10)\n",
      "The loss is 14749.2724609375 c shape (128, 10)\n",
      "The loss is 14176.142578125 c shape (128, 10)\n",
      "The loss is 15139.734375 c shape (128, 10)\n",
      "The loss is 15046.4599609375 c shape (128, 10)\n",
      "The loss is 14910.3037109375 c shape (128, 10)\n",
      "The loss is 14297.359375 c shape (128, 10)\n",
      "The loss is 14946.501953125 c shape (128, 10)\n",
      "The loss is 14508.8095703125 c shape (128, 10)\n",
      "The loss is 14507.6494140625 c shape (128, 10)\n",
      "The loss is 14566.41015625 c shape (128, 10)\n",
      "The loss is 14527.267578125 c shape (128, 10)\n",
      "The loss is 14892.798828125 c shape (128, 10)\n",
      "The loss is 13945.341796875 c shape (128, 10)\n",
      "The loss is 14980.05859375 c shape (128, 10)\n",
      "The loss is 14523.625 c shape (128, 10)\n",
      "The loss is 13939.5166015625 c shape (128, 10)\n",
      "The loss is 13837.4482421875 c shape (128, 10)\n",
      "The loss is 14204.2626953125 c shape (128, 10)\n",
      "The loss is 14307.29296875 c shape (128, 10)\n",
      "The loss is 13982.0703125 c shape (128, 10)\n",
      "The loss is 14614.146484375 c shape (128, 10)\n",
      "The loss is 13758.0224609375 c shape (128, 10)\n",
      "The loss is 14291.6640625 c shape (128, 10)\n",
      "The loss is 14316.333984375 c shape (128, 10)\n",
      "The loss is 14257.0908203125 c shape (128, 10)\n",
      "The loss is 13862.7705078125 c shape (128, 10)\n",
      "The loss is 14573.19140625 c shape (128, 10)\n",
      "The loss is 14569.8203125 c shape (128, 10)\n",
      "The loss is 14941.0205078125 c shape (128, 10)\n",
      "The loss is 14773.1806640625 c shape (128, 10)\n",
      "The loss is 14418.98828125 c shape (128, 10)\n",
      "The loss is 14056.81640625 c shape (128, 10)\n",
      "The loss is 13890.3359375 c shape (128, 10)\n",
      "The loss is 14606.7998046875 c shape (128, 10)\n",
      "The loss is 14350.93359375 c shape (128, 10)\n",
      "The loss is 14261.021484375 c shape (128, 10)\n",
      "The loss is 14339.685546875 c shape (128, 10)\n",
      "The loss is 14150.3310546875 c shape (128, 10)\n",
      "The loss is 14256.1572265625 c shape (128, 10)\n",
      "The loss is 14110.0859375 c shape (128, 10)\n",
      "The loss is 13681.84765625 c shape (128, 10)\n",
      "The loss is 14531.9521484375 c shape (128, 10)\n",
      "The loss is 14078.03515625 c shape (128, 10)\n",
      "The loss is 13591.9111328125 c shape (128, 10)\n",
      "The loss is 13594.3291015625 c shape (128, 10)\n",
      "The loss is 14932.6171875 c shape (128, 10)\n",
      "The loss is 14329.220703125 c shape (128, 10)\n",
      "The loss is 14113.9326171875 c shape (128, 10)\n",
      "The loss is 14143.8984375 c shape (128, 10)\n",
      "The loss is 13340.6796875 c shape (128, 10)\n",
      "The loss is 14074.375 c shape (128, 10)\n",
      "The loss is 13146.837890625 c shape (128, 10)\n",
      "The loss is 13767.7353515625 c shape (128, 10)\n",
      "The loss is 13857.2431640625 c shape (128, 10)\n",
      "The loss is 13230.140625 c shape (128, 10)\n",
      "The loss is 14232.4228515625 c shape (128, 10)\n",
      "The loss is 14172.837890625 c shape (128, 10)\n",
      "The loss is 14295.650390625 c shape (128, 10)\n",
      "The loss is 14387.8994140625 c shape (128, 10)\n",
      "The loss is 13900.9931640625 c shape (128, 10)\n",
      "The loss is 13575.828125 c shape (128, 10)\n",
      "The loss is 14138.501953125 c shape (128, 10)\n",
      "The loss is 14178.5634765625 c shape (128, 10)\n",
      "The loss is 14058.02734375 c shape (128, 10)\n",
      "The loss is 13841.68359375 c shape (128, 10)\n",
      "The loss is 13574.5078125 c shape (128, 10)\n",
      "The loss is 14565.810546875 c shape (128, 10)\n",
      "The loss is 13403.74609375 c shape (128, 10)\n",
      "The loss is 14388.353515625 c shape (128, 10)\n",
      "The loss is 13681.1611328125 c shape (128, 10)\n",
      "The loss is 14716.12109375 c shape (128, 10)\n",
      "The loss is 13905.7890625 c shape (128, 10)\n",
      "The loss is 13463.7958984375 c shape (128, 10)\n",
      "The loss is 13580.615234375 c shape (128, 10)\n",
      "The loss is 14165.6513671875 c shape (128, 10)\n",
      "The loss is 13296.03515625 c shape (128, 10)\n",
      "The loss is 13373.2958984375 c shape (128, 10)\n",
      "The loss is 13910.994140625 c shape (128, 10)\n",
      "The loss is 13469.3701171875 c shape (128, 10)\n",
      "The loss is 13520.36328125 c shape (128, 10)\n",
      "The loss is 14099.6494140625 c shape (128, 10)\n",
      "The loss is 14016.916015625 c shape (128, 10)\n",
      "2th iteration ===========================================================================\n",
      "The loss is 12687.9873046875 c shape (128, 10)\n",
      "The loss is 13809.7685546875 c shape (128, 10)\n",
      "The loss is 13950.181640625 c shape (128, 10)\n",
      "The loss is 14036.021484375 c shape (128, 10)\n",
      "The loss is 13483.740234375 c shape (128, 10)\n",
      "The loss is 13531.787109375 c shape (128, 10)\n",
      "The loss is 13441.4638671875 c shape (128, 10)\n",
      "The loss is 13543.705078125 c shape (128, 10)\n",
      "The loss is 13351.8349609375 c shape (128, 10)\n",
      "The loss is 12826.41796875 c shape (128, 10)\n",
      "The loss is 13653.201171875 c shape (128, 10)\n",
      "The loss is 13180.583984375 c shape (128, 10)\n",
      "The loss is 14241.658203125 c shape (128, 10)\n",
      "The loss is 13525.083984375 c shape (128, 10)\n",
      "The loss is 13217.537109375 c shape (128, 10)\n",
      "The loss is 13583.408203125 c shape (128, 10)\n",
      "The loss is 13979.5634765625 c shape (128, 10)\n",
      "The loss is 13732.212890625 c shape (128, 10)\n",
      "The loss is 13359.6337890625 c shape (128, 10)\n",
      "The loss is 13202.572265625 c shape (128, 10)\n",
      "The loss is 13111.3251953125 c shape (128, 10)\n",
      "The loss is 13229.5625 c shape (128, 10)\n",
      "The loss is 13659.0634765625 c shape (128, 10)\n",
      "The loss is 13485.6796875 c shape (128, 10)\n",
      "The loss is 13902.5908203125 c shape (128, 10)\n",
      "The loss is 13383.33203125 c shape (128, 10)\n",
      "The loss is 13373.12109375 c shape (128, 10)\n",
      "The loss is 13776.91796875 c shape (128, 10)\n",
      "The loss is 13324.5126953125 c shape (128, 10)\n",
      "The loss is 14053.341796875 c shape (128, 10)\n",
      "The loss is 13221.9541015625 c shape (128, 10)\n",
      "The loss is 13470.6923828125 c shape (128, 10)\n",
      "The loss is 14182.56640625 c shape (128, 10)\n",
      "The loss is 14084.5966796875 c shape (128, 10)\n",
      "The loss is 13725.732421875 c shape (128, 10)\n",
      "The loss is 12970.68359375 c shape (128, 10)\n",
      "The loss is 12515.181640625 c shape (128, 10)\n",
      "The loss is 13793.4296875 c shape (128, 10)\n",
      "The loss is 13197.8994140625 c shape (128, 10)\n",
      "The loss is 13841.6767578125 c shape (128, 10)\n",
      "The loss is 13486.5380859375 c shape (128, 10)\n",
      "The loss is 13647.5712890625 c shape (128, 10)\n",
      "The loss is 13818.1650390625 c shape (128, 10)\n",
      "The loss is 13324.619140625 c shape (128, 10)\n",
      "The loss is 13198.30078125 c shape (128, 10)\n",
      "The loss is 13750.8515625 c shape (128, 10)\n",
      "The loss is 12172.55078125 c shape (128, 10)\n",
      "The loss is 12353.7861328125 c shape (128, 10)\n",
      "The loss is 13585.9638671875 c shape (128, 10)\n",
      "The loss is 13131.7275390625 c shape (128, 10)\n",
      "The loss is 13163.599609375 c shape (128, 10)\n",
      "The loss is 13263.3515625 c shape (128, 10)\n",
      "The loss is 13419.1162109375 c shape (128, 10)\n",
      "The loss is 12755.642578125 c shape (128, 10)\n",
      "The loss is 13246.5732421875 c shape (128, 10)\n",
      "The loss is 12227.3515625 c shape (128, 10)\n",
      "The loss is 13589.138671875 c shape (128, 10)\n",
      "The loss is 13314.5322265625 c shape (128, 10)\n",
      "The loss is 13453.78125 c shape (128, 10)\n",
      "The loss is 13326.615234375 c shape (128, 10)\n",
      "The loss is 12973.8408203125 c shape (128, 10)\n",
      "The loss is 13221.728515625 c shape (128, 10)\n",
      "The loss is 13328.7451171875 c shape (128, 10)\n",
      "The loss is 13479.7041015625 c shape (128, 10)\n",
      "The loss is 12985.6318359375 c shape (128, 10)\n",
      "The loss is 13516.2822265625 c shape (128, 10)\n",
      "The loss is 13419.0859375 c shape (128, 10)\n",
      "The loss is 13486.900390625 c shape (128, 10)\n",
      "The loss is 12953.1328125 c shape (128, 10)\n",
      "The loss is 13669.0810546875 c shape (128, 10)\n",
      "The loss is 12713.3056640625 c shape (128, 10)\n",
      "The loss is 13390.34765625 c shape (128, 10)\n",
      "The loss is 13601.79296875 c shape (128, 10)\n",
      "The loss is 13477.423828125 c shape (128, 10)\n",
      "The loss is 13497.0146484375 c shape (128, 10)\n",
      "The loss is 12264.7236328125 c shape (128, 10)\n",
      "The loss is 12893.896484375 c shape (128, 10)\n",
      "The loss is 12863.580078125 c shape (128, 10)\n",
      "The loss is 13743.736328125 c shape (128, 10)\n",
      "The loss is 13157.8994140625 c shape (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 13787.716796875 c shape (128, 10)\n",
      "The loss is 12646.875 c shape (128, 10)\n",
      "The loss is 12329.6328125 c shape (128, 10)\n",
      "The loss is 13086.048828125 c shape (128, 10)\n",
      "The loss is 12565.703125 c shape (128, 10)\n",
      "The loss is 13027.333984375 c shape (128, 10)\n",
      "The loss is 12636.751953125 c shape (128, 10)\n",
      "The loss is 12172.294921875 c shape (128, 10)\n",
      "The loss is 12141.576171875 c shape (128, 10)\n",
      "The loss is 12821.28515625 c shape (128, 10)\n",
      "The loss is 13014.337890625 c shape (128, 10)\n",
      "The loss is 13319.04296875 c shape (128, 10)\n",
      "The loss is 12461.2001953125 c shape (128, 10)\n",
      "The loss is 12606.9375 c shape (128, 10)\n",
      "The loss is 13210.025390625 c shape (128, 10)\n",
      "The loss is 13314.54296875 c shape (128, 10)\n",
      "The loss is 13419.19921875 c shape (128, 10)\n",
      "The loss is 13066.5771484375 c shape (128, 10)\n",
      "The loss is 13674.13671875 c shape (128, 10)\n",
      "The loss is 13191.162109375 c shape (128, 10)\n",
      "The loss is 13117.185546875 c shape (128, 10)\n",
      "The loss is 12917.8349609375 c shape (128, 10)\n",
      "The loss is 12975.763671875 c shape (128, 10)\n",
      "The loss is 13752.513671875 c shape (128, 10)\n",
      "The loss is 13586.0439453125 c shape (128, 10)\n",
      "The loss is 13496.115234375 c shape (128, 10)\n",
      "The loss is 13254.4697265625 c shape (128, 10)\n",
      "The loss is 13264.0283203125 c shape (128, 10)\n",
      "The loss is 13597.33203125 c shape (128, 10)\n",
      "The loss is 12927.8603515625 c shape (128, 10)\n",
      "The loss is 13306.83203125 c shape (128, 10)\n",
      "The loss is 13128.5986328125 c shape (128, 10)\n",
      "The loss is 12720.1640625 c shape (128, 10)\n",
      "The loss is 13094.8564453125 c shape (128, 10)\n",
      "The loss is 13295.37890625 c shape (128, 10)\n",
      "The loss is 13120.9765625 c shape (128, 10)\n",
      "The loss is 13226.544921875 c shape (128, 10)\n",
      "The loss is 12990.81640625 c shape (128, 10)\n",
      "The loss is 13442.626953125 c shape (128, 10)\n",
      "The loss is 12782.208984375 c shape (128, 10)\n",
      "The loss is 13028.353515625 c shape (128, 10)\n",
      "The loss is 12540.5673828125 c shape (128, 10)\n",
      "The loss is 12641.9423828125 c shape (128, 10)\n",
      "The loss is 12886.431640625 c shape (128, 10)\n",
      "The loss is 12297.57421875 c shape (128, 10)\n",
      "The loss is 13504.087890625 c shape (128, 10)\n",
      "The loss is 13209.8828125 c shape (128, 10)\n",
      "The loss is 13314.27734375 c shape (128, 10)\n",
      "The loss is 13160.087890625 c shape (128, 10)\n",
      "The loss is 13201.32421875 c shape (128, 10)\n",
      "The loss is 13287.515625 c shape (128, 10)\n",
      "The loss is 12647.9111328125 c shape (128, 10)\n",
      "The loss is 12605.421875 c shape (128, 10)\n",
      "The loss is 13165.869140625 c shape (128, 10)\n",
      "The loss is 13716.615234375 c shape (128, 10)\n",
      "The loss is 12626.5283203125 c shape (128, 10)\n",
      "The loss is 13046.0703125 c shape (128, 10)\n",
      "The loss is 12931.748046875 c shape (128, 10)\n",
      "The loss is 12636.73046875 c shape (128, 10)\n",
      "The loss is 13475.3603515625 c shape (128, 10)\n",
      "The loss is 12257.607421875 c shape (128, 10)\n",
      "The loss is 13038.19921875 c shape (128, 10)\n",
      "The loss is 13083.728515625 c shape (128, 10)\n",
      "The loss is 13017.154296875 c shape (128, 10)\n",
      "The loss is 12344.001953125 c shape (128, 10)\n",
      "The loss is 13020.6591796875 c shape (128, 10)\n",
      "The loss is 12558.9375 c shape (128, 10)\n",
      "The loss is 12351.0810546875 c shape (128, 10)\n",
      "The loss is 13350.8720703125 c shape (128, 10)\n",
      "The loss is 12974.783203125 c shape (128, 10)\n",
      "The loss is 13012.529296875 c shape (128, 10)\n",
      "The loss is 12878.083984375 c shape (128, 10)\n",
      "The loss is 12357.5078125 c shape (128, 10)\n",
      "The loss is 12228.11328125 c shape (128, 10)\n",
      "The loss is 12893.6064453125 c shape (128, 10)\n",
      "The loss is 12047.5625 c shape (128, 10)\n",
      "The loss is 12255.0439453125 c shape (128, 10)\n",
      "The loss is 13033.16796875 c shape (128, 10)\n",
      "The loss is 13187.3056640625 c shape (128, 10)\n",
      "The loss is 12865.94921875 c shape (128, 10)\n",
      "The loss is 13023.58984375 c shape (128, 10)\n",
      "The loss is 12680.076171875 c shape (128, 10)\n",
      "The loss is 12512.8759765625 c shape (128, 10)\n",
      "The loss is 12405.0986328125 c shape (128, 10)\n",
      "The loss is 12608.1513671875 c shape (128, 10)\n",
      "The loss is 12876.22265625 c shape (128, 10)\n",
      "The loss is 13277.0390625 c shape (128, 10)\n",
      "The loss is 12949.9375 c shape (128, 10)\n",
      "The loss is 13501.2041015625 c shape (128, 10)\n",
      "The loss is 12768.7900390625 c shape (128, 10)\n",
      "The loss is 13809.236328125 c shape (128, 10)\n",
      "The loss is 13063.2939453125 c shape (128, 10)\n",
      "The loss is 12385.3994140625 c shape (128, 10)\n",
      "The loss is 12535.8359375 c shape (128, 10)\n",
      "The loss is 12691.39453125 c shape (128, 10)\n",
      "The loss is 12930.68359375 c shape (128, 10)\n",
      "The loss is 12950.2998046875 c shape (128, 10)\n",
      "The loss is 12691.765625 c shape (128, 10)\n",
      "The loss is 12933.0087890625 c shape (128, 10)\n",
      "The loss is 13290.7705078125 c shape (128, 10)\n",
      "The loss is 12642.3134765625 c shape (128, 10)\n",
      "The loss is 12262.1845703125 c shape (128, 10)\n",
      "The loss is 12023.8955078125 c shape (128, 10)\n",
      "The loss is 12431.3515625 c shape (128, 10)\n",
      "The loss is 12874.1708984375 c shape (128, 10)\n",
      "The loss is 13073.564453125 c shape (128, 10)\n",
      "The loss is 12148.5625 c shape (128, 10)\n",
      "The loss is 12584.5673828125 c shape (128, 10)\n",
      "The loss is 11856.123046875 c shape (128, 10)\n",
      "The loss is 12628.79296875 c shape (128, 10)\n",
      "The loss is 12161.1728515625 c shape (128, 10)\n",
      "The loss is 12664.6259765625 c shape (128, 10)\n",
      "The loss is 12263.876953125 c shape (128, 10)\n",
      "The loss is 12568.646484375 c shape (128, 10)\n",
      "The loss is 12155.818359375 c shape (128, 10)\n",
      "The loss is 12531.9375 c shape (128, 10)\n",
      "The loss is 12134.40234375 c shape (128, 10)\n",
      "The loss is 12099.5703125 c shape (128, 10)\n",
      "The loss is 12493.509765625 c shape (128, 10)\n",
      "The loss is 12133.7421875 c shape (128, 10)\n",
      "The loss is 11836.646484375 c shape (128, 10)\n",
      "The loss is 12125.5693359375 c shape (128, 10)\n",
      "The loss is 12524.2548828125 c shape (128, 10)\n",
      "The loss is 11854.806640625 c shape (128, 10)\n",
      "The loss is 12366.73828125 c shape (128, 10)\n",
      "The loss is 12373.87109375 c shape (128, 10)\n",
      "The loss is 13051.044921875 c shape (128, 10)\n",
      "The loss is 12741.0302734375 c shape (128, 10)\n",
      "The loss is 13117.087890625 c shape (128, 10)\n",
      "The loss is 12974.115234375 c shape (128, 10)\n",
      "The loss is 12329.0556640625 c shape (128, 10)\n",
      "The loss is 12416.73828125 c shape (128, 10)\n",
      "The loss is 11902.619140625 c shape (128, 10)\n",
      "The loss is 12393.21484375 c shape (128, 10)\n",
      "The loss is 11824.5859375 c shape (128, 10)\n",
      "The loss is 12258.69921875 c shape (128, 10)\n",
      "The loss is 12518.693359375 c shape (128, 10)\n",
      "The loss is 12796.513671875 c shape (128, 10)\n",
      "The loss is 12615.69140625 c shape (128, 10)\n",
      "The loss is 12435.4638671875 c shape (128, 10)\n",
      "The loss is 12828.5732421875 c shape (128, 10)\n",
      "The loss is 12630.923828125 c shape (128, 10)\n",
      "The loss is 13156.927734375 c shape (128, 10)\n",
      "The loss is 12695.3291015625 c shape (128, 10)\n",
      "The loss is 11987.828125 c shape (128, 10)\n",
      "The loss is 12062.2412109375 c shape (128, 10)\n",
      "The loss is 12257.0146484375 c shape (128, 10)\n",
      "The loss is 12138.3330078125 c shape (128, 10)\n",
      "The loss is 12481.8330078125 c shape (128, 10)\n",
      "The loss is 12555.220703125 c shape (128, 10)\n",
      "The loss is 12168.357421875 c shape (128, 10)\n",
      "The loss is 12663.689453125 c shape (128, 10)\n",
      "The loss is 12948.697265625 c shape (128, 10)\n",
      "The loss is 12591.8984375 c shape (128, 10)\n",
      "The loss is 12644.001953125 c shape (128, 10)\n",
      "The loss is 12317.427734375 c shape (128, 10)\n",
      "The loss is 12580.5107421875 c shape (128, 10)\n",
      "The loss is 12734.033203125 c shape (128, 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth iteration ===========================================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCVA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mtrain_vae\u001b[0;34m(model, train_loader, args, is_cvae)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     33\u001b[0m         x_hat, mu, logvar \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcall(x,c)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(labels, class_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCreate one hot label matrix of size (N, C)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mtargets[i, j + 1:] are equal to 0\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], class_size))\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels):\n\u001b[1;32m     16\u001b[0m     targets[i, label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m targets \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(targets)\n",
      "File \u001b[0;32m~/Desktop/CSIC 2470/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7010\u001b[0m, in \u001b[0;36m_TensorIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limit:\n\u001b[1;32m   7009\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m-> 7010\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   7011\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   7012\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/CSIC 2470/env/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/Desktop/CSIC 2470/env/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1013\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1011\u001b[0m   new_axis_mask \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m index)\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1013\u001b[0m   \u001b[43m_check_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m   begin\u001b[38;5;241m.\u001b[39mappend(s)\n\u001b[1;32m   1015\u001b[0m   end\u001b[38;5;241m.\u001b[39mappend(s \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CSIC 2470/env/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:876\u001b[0m, in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_index\u001b[39m(idx):\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;124;03m\"\"\"Check if a given value is a valid index into a tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 876\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumbers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIntegral\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDimension\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    879\u001b[0m   \u001b[38;5;66;03m# Optimistic check. Assumptions:\u001b[39;00m\n\u001b[1;32m    880\u001b[0m   \u001b[38;5;66;03m# * any object with a dtype is supported\u001b[39;00m\n\u001b[1;32m    881\u001b[0m   \u001b[38;5;66;03m# * any object with a dtype has a sizeable shape attribute.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/abc.py:119\u001b[0m, in \u001b[0;36mABCMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124;03m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_abc_instancecheck\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/abc.py:123\u001b[0m, in \u001b[0;36mABCMeta.__subclasscheck__\u001b[0;34m(cls, subclass)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__subclasscheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, subclass):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124;03m\"\"\"Override for issubclass(subclass, cls).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_abc_subclasscheck\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubclass\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CVA = CVAE(784)\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}th iteration ===========================================================================\")\n",
    "    train_vae(CVA,load_mnist(128, buffer_size=1024) , 1, is_cvae=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "86ffa5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.round(1.22,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e4a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a536ffc3",
   "metadata": {},
   "source": [
    "## Others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d204ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
